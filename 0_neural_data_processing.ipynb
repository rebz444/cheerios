{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import constants as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pickle_name = \"session_keys_0120\"\n",
    "data_dir = '/Users/rebekahzhang/data/neural_data'\n",
    "pickle_dir = Path(os.path.join(data_dir, 'session_pickles'))\n",
    "figure_folder = os.path.join(data_dir, 'figures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify sorted data and recording log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load sorted session data pickle**\n",
    "- pickle is a list of dictionaries\n",
    "- each dict representing one session, with subject, session date time, a event df, and a list of units\n",
    "- each unit is an array of spike times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, f'{raw_pickle_name}.pkl'), 'rb') as f:\n",
    "    sorted_sessions_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**make a log converting the data list into a df**\n",
    "- temporarily keeping events and units in the df. \n",
    "- will drop later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sessions_sorted(sorted_sessions_list):\n",
    "    session_info_list = []\n",
    "    for session in sorted_sessions_list:\n",
    "        session_info = {\n",
    "            'mouse': session['subject'],\n",
    "            'datetime': session['session_datetime'],\n",
    "            'date': session['session_datetime'].strftime(\"%Y-%m-%d\"),\n",
    "            'insertion_number': session['insertion_number'],\n",
    "            'paramset_idx': session['paramset_idx'],\n",
    "            'num_units': len(session['spikes']),\n",
    "            'events': session['events'],\n",
    "            'units': session['spikes']\n",
    "        }\n",
    "        session_info_list.append(session_info)\n",
    "    session_info_df = pd.DataFrame(session_info_list)\n",
    "    return session_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sessions = generate_sessions_sorted(sorted_sessions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 sessions with units\n",
      "total cells: 1385\n"
     ]
    }
   ],
   "source": [
    "sorted_sessions_with_units = sorted_sessions.loc[sorted_sessions['num_units'] > 0]\n",
    "\n",
    "print(len(sorted_sessions_with_units), \"sessions with units\")\n",
    "print(\"total cells:\", sum(sorted_sessions_with_units.num_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load recording log**\n",
    "- download from google sheet and save it to data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_log = pd.read_csv(os.path.join(data_dir, 'recording_log.csv'))\n",
    "recording_log = recording_log.drop(columns=['NIDAQ', 'simultaneous', 'probe', 'probe treatment', 'insertion speed',\n",
    "       'resting time', 'surface', 'extraction speed', 'notes', 'rewards',\n",
    "       'num trials', 'tw', 'potential problems', 'sorting notes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**merge the two df to add region info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_session_all = pd.merge(\n",
    "    recording_log, sorted_sessions,\n",
    "    on=['mouse', 'date', 'insertion_number'],\n",
    "    how='inner'\n",
    ")\n",
    "sorted_session_all['id'] = sorted_session_all[['mouse', 'date', 'region']].agg('_'.join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process sorted session data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### events processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_events(events_df):\n",
    "    # Get trial start times (assuming one 'trial' event per trial_id)\n",
    "    trial_starts = events_df.loc[events_df['event_type'] == 'trial']\n",
    "    trial_starts = trial_starts.set_index('trial_id')['event_start_time']\n",
    "    \n",
    "    # Map trial start times to all events\n",
    "    events_df['trial_start_time'] = events_df['trial_id'].map(trial_starts)\n",
    "    \n",
    "    # Calculate relative times\n",
    "    events_df['event_start_trial_time'] = events_df['event_start_time'] - events_df['trial_start_time']\n",
    "    events_df['event_end_trial_time'] = events_df['event_end_time'] - events_df['trial_start_time']\n",
    "    \n",
    "    # Drop the column cuz all values are 0\n",
    "    events_df = events_df.drop(columns=['trial_start_time'])\n",
    "    \n",
    "    return events_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trials processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trial_data(trial):\n",
    "    # Extract visual cue times\n",
    "    visual_events = trial[trial['event_type'] == 'visual']\n",
    "    cue_on_time = visual_events['event_start_trial_time'].iloc[0]\n",
    "    cue_off_time = visual_events['event_end_trial_time'].iloc[0]\n",
    "    \n",
    "    # Initialize default values\n",
    "    trial_data = {\n",
    "        'missed': True,  # default assumption\n",
    "        'rewarded': False,\n",
    "        'cue_on_time': cue_on_time,\n",
    "        'cue_off_time': cue_off_time,\n",
    "        'consumption_time': np.nan,\n",
    "        'background_length': cue_off_time - cue_on_time,\n",
    "        'wait_length': 60,  # default for missed trials\n",
    "    }\n",
    "    \n",
    "    # Check for reward events\n",
    "    if 'reward' in trial['event_type'].values:\n",
    "        reward_time = trial.loc[trial['event_type'] == 'reward', 'event_start_trial_time'].iloc[0]\n",
    "        trial_data.update({\n",
    "            'missed': False,\n",
    "            'wait_length': reward_time - cue_off_time\n",
    "        })\n",
    "        \n",
    "        # Check for consumption events\n",
    "        for cons_type in ['cons_reward', 'cons_no_reward']:\n",
    "            if cons_type in trial['event_type'].values:\n",
    "                trial_data.update({\n",
    "                    'consumption_time': trial.loc[trial['event_type'] == cons_type, 'event_start_trial_time'].iloc[0],\n",
    "                    'rewarded': (cons_type == 'cons_reward')\n",
    "                })\n",
    "    \n",
    "    return trial_data\n",
    "\n",
    "def generate_trials(events):\n",
    "    trials = events.loc[events['event_type'] == 'trial'].copy()\n",
    "    trial_data_list = []\n",
    "    for t, trial in events.groupby(\"trial_id\"):\n",
    "        trial_data = {'trial_id': t} | get_trial_data(trial)\n",
    "        trial_data_list.append(trial_data)\n",
    "    trial_data_df = pd.DataFrame(trial_data_list)\n",
    "    trials = pd.merge(trials, trial_data_df, on='trial_id')\n",
    "    trials['consumption_length'] = trials['event_end_trial_time'] - trials['consumption_time']\n",
    "    trials = trials.rename(columns={\"event_end_trial_time\": \"trial_length\"})\n",
    "\n",
    "    trials = trials.drop(columns=['event_start_trial_time', 'event_type'])\n",
    "    return trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spikes processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trial_time_to_spikes(spikes, trials):\n",
    "    for _, trial_basics in trials.iterrows():\n",
    "        trial_start_time = trial_basics['event_start_time']\n",
    "        trial_end_time = trial_basics['event_end_time']\n",
    "        spikes.loc[spikes['spike_time'].between(trial_start_time, trial_end_time), \n",
    "                'trial_id'] = trial_basics['trial_id']\n",
    "        spikes.loc[spikes['spike_time'].between(trial_start_time, trial_end_time), \n",
    "                'trial_time'] = spikes['spike_time'] - trial_start_time\n",
    "        time_columns = [\"cue_on_time\", \"cue_off_time\", \"consumption_time\"] \n",
    "    trials_to_merge = trials[['trial_id']+ time_columns].copy()\n",
    "    spikes = trials_to_merge.merge(spikes, on='trial_id', how='inner')\n",
    "    return spikes\n",
    "\n",
    "def align_spike_time_to_anchors(spikes):\n",
    "    spikes[k.TO_CUE_ON] = spikes['trial_time'] - spikes[\"cue_on_time\"]\n",
    "    spikes[k.TO_CUE_OFF] = spikes['trial_time'] - spikes[\"cue_off_time\"]\n",
    "    spikes[k.TO_CONSUMPTION] = spikes['trial_time'] - spikes[\"consumption_time\"]    \n",
    "    return spikes\n",
    "\n",
    "def add_period_to_spikes(row):\n",
    "    if row['cue_on_time'] <= row['trial_time'] < row['cue_off_time']:\n",
    "        return k.BACKGROUND\n",
    "    elif row['cue_off_time'] <= row['trial_time'] < row['consumption_time']:\n",
    "        return k.WAIT\n",
    "    elif row['consumption_time'] <= row['trial_time']:\n",
    "        return k.CONSUMPTION\n",
    "\n",
    "def process_spikes(spikes, trials):\n",
    "    spikes = pd.DataFrame(spikes, columns=['spike_time'])\n",
    "    spikes = add_trial_time_to_spikes(spikes, trials)\n",
    "    spikes = align_spike_time_to_anchors(spikes)\n",
    "    spikes['period'] = spikes.apply(add_period_to_spikes, axis=1)\n",
    "    return spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### align events to anchor times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_events(events, trials):\n",
    "    time_columns = [\"cue_on_time\", \"cue_off_time\", \"consumption_time\"]\n",
    "    trials_to_merge = trials[['trial_id']+ time_columns].copy()\n",
    "\n",
    "    events = trials_to_merge.merge(events, on='trial_id', how='inner')\n",
    "    events[k.TO_CUE_ON] = events['event_start_trial_time'] - events[\"cue_on_time\"]\n",
    "    events[k.TO_CUE_OFF] = events['event_start_trial_time'] - events[\"cue_off_time\"]\n",
    "    events[k.TO_CONSUMPTION] = events['event_start_trial_time'] - events[\"consumption_time\"]\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process a session through all steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_session(session):\n",
    "    \"\"\"session is a row in the df\"\"\"\n",
    "    events = process_raw_events(session['events'])\n",
    "    trials = generate_trials(events)\n",
    "    events_aligned = align_events(events, trials)\n",
    "    units_aligned = [process_spikes(unit, trials) for unit in session['units']]\n",
    "    return events_aligned, trials, units_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging\n",
    "# test_log = sorted_session_all.head(2)\n",
    "# test_session = sorted_session_all.iloc[6]\n",
    "# test_events, test_trials, test_units = process_session(test_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop through all sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_all_sessions(sorted_session_all, pickle_dir, regenerate):\n",
    "    Path(pickle_dir).mkdir(parents=True, exist_ok=True)\n",
    "    for _, session in sorted_session_all.iterrows():\n",
    "        output_path = os.path.join(pickle_dir, f\"{session['id']}.pkl\")\n",
    "                # Skip if file exists and we're not regenerating\n",
    "        if os.path.exists(output_path) and not regenerate:\n",
    "            print(f\"Session {session['id']} already exists at - skipping\")\n",
    "            continue\n",
    "\n",
    "        events, trials, units = process_session(session)\n",
    "\n",
    "        session_data = {\n",
    "            'id' : session['id'],\n",
    "            'mouse': session['mouse'],\n",
    "            'date': session['date'],\n",
    "            'region': session['region'],\n",
    "            'events': events,\n",
    "            'trials': trials,\n",
    "            'units': units,\n",
    "        }\n",
    "\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(session_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        print(f\"Saved session {session['id']} to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session RZ034_2024-07-13_v1 already exists at - skipping\n",
      "Session RZ034_2024-07-13_str already exists at - skipping\n",
      "Session RZ034_2024-07-14_str already exists at - skipping\n",
      "Session RZ036_2024-07-13_v1 already exists at - skipping\n",
      "Session RZ036_2024-07-13_str already exists at - skipping\n",
      "Session RZ036_2024-07-14_str already exists at - skipping\n",
      "Session RZ037_2024-07-16_v1 already exists at - skipping\n",
      "Session RZ037_2024-07-16_str already exists at - skipping\n",
      "Session RZ037_2024-07-17_v1 already exists at - skipping\n",
      "Session RZ037_2024-07-17_str already exists at - skipping\n",
      "Session RZ038_2024-07-16_v1 already exists at - skipping\n",
      "Session RZ038_2024-07-16_str already exists at - skipping\n",
      "Session RZ038_2024-07-18_str already exists at - skipping\n",
      "Session RZ038_2024-07-19_str already exists at - skipping\n",
      "Session RZ039_2024-07-17_str already exists at - skipping\n",
      "Session RZ053_2024-10-22_v1 already exists at - skipping\n",
      "Session RZ036_2024-07-12_v1 already exists at - skipping\n",
      "Session RZ036_2024-07-12_str already exists at - skipping\n",
      "Session RZ037_2024-07-18_v1 already exists at - skipping\n",
      "Session RZ037_2024-07-18_str already exists at - skipping\n",
      "Session RZ038_2024-07-17_v1 already exists at - skipping\n",
      "Session RZ038_2024-07-17_str already exists at - skipping\n",
      "Session RZ047_2024-11-19_v1 already exists at - skipping\n",
      "Session RZ047_2024-11-19_str already exists at - skipping\n",
      "Session RZ047_2024-11-20_v1 already exists at - skipping\n",
      "Session RZ047_2024-11-20_str already exists at - skipping\n",
      "Session RZ047_2024-11-21_v1 already exists at - skipping\n",
      "Session RZ047_2024-11-21_str already exists at - skipping\n",
      "Session RZ047_2024-11-22_v1 already exists at - skipping\n",
      "Session RZ047_2024-11-22_str already exists at - skipping\n",
      "Session RZ049_2024-10-29_v1 already exists at - skipping\n",
      "Session RZ049_2024-10-29_str already exists at - skipping\n",
      "Session RZ049_2024-10-30_v1 already exists at - skipping\n",
      "Session RZ049_2024-10-30_str already exists at - skipping\n",
      "Session RZ049_2024-10-31_v1 already exists at - skipping\n",
      "Session RZ049_2024-11-01_v1 already exists at - skipping\n",
      "Session RZ049_2024-11-01_str already exists at - skipping\n",
      "Session RZ050_2024-11-19_v1 already exists at - skipping\n",
      "Session RZ050_2024-11-19_str already exists at - skipping\n",
      "Session RZ050_2024-11-20_v1 already exists at - skipping\n",
      "Session RZ050_2024-11-20_str already exists at - skipping\n",
      "Session RZ050_2024-11-21_v1 already exists at - skipping\n",
      "Session RZ050_2024-11-21_str already exists at - skipping\n",
      "Session RZ050_2024-11-22_v1 already exists at - skipping\n",
      "Session RZ050_2024-11-22_str already exists at - skipping\n",
      "Session RZ051_2024-11-19_v1 already exists at - skipping\n",
      "Session RZ051_2024-11-19_str already exists at - skipping\n",
      "Session RZ051_2024-11-20_v1 already exists at - skipping\n",
      "Session RZ051_2024-11-20_str already exists at - skipping\n",
      "Session RZ051_2024-11-21_v1 already exists at - skipping\n",
      "Session RZ051_2024-11-21_str already exists at - skipping\n",
      "Session RZ051_2024-11-22_v1 already exists at - skipping\n",
      "Session RZ051_2024-11-22_str already exists at - skipping\n",
      "Session RZ052_2024-10-25_v1 already exists at - skipping\n",
      "Session RZ053_2024-10-24_v1 already exists at - skipping\n",
      "Session RZ053_2024-10-25_v1 already exists at - skipping\n",
      "Session RZ055_2024-10-30_v1 already exists at - skipping\n",
      "Session RZ055_2024-10-30_str already exists at - skipping\n",
      "Session RZ055_2024-10-31_v1 already exists at - skipping\n",
      "Session RZ055_2024-10-31_str already exists at - skipping\n",
      "Session RZ055_2024-11-01_v1 already exists at - skipping\n"
     ]
    }
   ],
   "source": [
    "regenerate = False\n",
    "process_and_save_all_sessions(sorted_session_all, pickle_dir, regenerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalize and save session log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_session_all = sorted_session_all.drop(columns=['events', 'units'])\n",
    "sorted_session_all.to_csv(os.path.join(data_dir, 'sorted_session_all.csv'))\n",
    "sorted_sessions_w_units = sorted_session_all.loc[sorted_session_all['num_units']>0]\n",
    "sorted_sessions_w_units.to_csv(os.path.join(data_dir, 'sorted_session_with_units.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CHEERIOS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
