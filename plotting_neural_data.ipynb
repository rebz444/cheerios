{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/rebekahzhang/data/neural_data'\n",
    "figure_folder = os.path.join(data_dir, 'figures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data pickle and filter out sessions with no units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the session_keys pickle. its a list of dictionaries, each dict representing one session, \n",
    "# with a event df, and a list of units, each unit is an array of spike times\n",
    "# with open(os.path.join(data_dir, 'session_keys_0120.pkl'), 'rb') as f:\n",
    "#     session_data_list = pickle.load(f)\n",
    "with open(os.path.join(data_dir, 'session_keys.pkl'), 'rb') as f:\n",
    "    session_data_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(session_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out the sessions with no units\n",
    "def get_seeions_with_units(session_data_list):\n",
    "    sessions_with_units = []\n",
    "    for session in session_data_list:\n",
    "        num_neurons = len(session['spikes'])\n",
    "        if num_neurons > 0:\n",
    "            sessions_with_units.append(session)\n",
    "    return sessions_with_units\n",
    "\n",
    "session_with_units = get_seeions_with_units(session_data_list)\n",
    "len(session_with_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RZ034 2024-07-14 12:52:46 31\n",
      "RZ036 2024-07-12 12:50:31 15\n",
      "RZ036 2024-07-12 12:50:31 45\n",
      "RZ036 2024-07-13 14:29:03 30\n",
      "RZ037 2024-07-16 11:33:07 61\n",
      "RZ037 2024-07-17 17:09:40 60\n",
      "RZ037 2024-07-18 12:39:17 13\n",
      "RZ037 2024-07-18 12:39:17 18\n",
      "RZ038 2024-07-17 12:01:45 2\n",
      "RZ038 2024-07-18 15:07:36 32\n",
      "RZ038 2024-07-19 12:38:14 19\n",
      "RZ039 2024-07-17 14:45:27 16\n",
      "total cells: 342\n"
     ]
    }
   ],
   "source": [
    "total_cell_count = 0\n",
    "for session in session_with_units:\n",
    "    total_cell_count+=len(session['spikes'])\n",
    "    print(session['subject'], session['session_datetime'], len(session['spikes']))\n",
    "print(\"total cells:\", total_cell_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session_identity(session_dict):\n",
    "    mouse = session_dict['subject']\n",
    "    date = session_dict['session_datetime'].strftime(\"%Y-%m-%d\")\n",
    "    insertion = session_dict['insertion_number']+1\n",
    "    session_identity = f'{mouse}_{date}_probe-{insertion}'\n",
    "    return session_identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add trial time to events\n",
    "each row is an event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trial_time(trial):\n",
    "    trial_start_time = trial.loc[trial['event_type'] == 'trial', 'event_start_time'].iloc[0]\n",
    "    trial['event_start_trial_time'] = trial['event_start_time'] - trial_start_time\n",
    "    trial['event_end_trial_time'] = trial['event_end_time'] - trial_start_time\n",
    "    return trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate trials and add trial data based on events\n",
    "each row is a trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trial_data(trial):\n",
    "    missed = False\n",
    "    rewarded = False\n",
    "    wait_length = np.nan\n",
    "    \n",
    "    cue_on_time = trial.loc[trial['event_type'] == 'visual', 'event_start_trial_time'].iloc[0]\n",
    "    cue_off_time = trial.loc[trial['event_type'] == 'visual', 'event_end_trial_time'].iloc[0]\n",
    "    cons_time = np.nan\n",
    "\n",
    "    if 'reward' in trial.event_type.unique():\n",
    "        missed = False\n",
    "        wait_length = trial.loc[trial['event_type'] == 'reward', 'event_start_trial_time'].iloc[0] - cue_off_time\n",
    "\n",
    "        if 'cons_reward' in trial.event_type.unique():\n",
    "            rewarded = True\n",
    "            cons_time = trial.loc[trial['event_type'] == 'cons_reward', 'event_start_trial_time'].iloc[0]\n",
    "        elif 'cons_no_reward' in trial.event_type.unique():\n",
    "            rewarded = False\n",
    "            cons_time = trial.loc[trial['event_type'] == 'cons_no_reward', 'event_start_trial_time'].iloc[0]\n",
    "\n",
    "    elif 'reward' not in trial.event_type.unique():\n",
    "        missed = True\n",
    "        wait_length = 60\n",
    "\n",
    "    trial_data = {\n",
    "        'missed': missed,\n",
    "        'rewarded': rewarded,\n",
    "        'cue_on_time': cue_on_time,\n",
    "        'cue_off_time': cue_off_time,\n",
    "        'cons_time': cons_time,\n",
    "        'bg_length': cue_off_time-cue_on_time,\n",
    "        'wait_length': wait_length,\n",
    "    }\n",
    "\n",
    "    return trial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trials(events):\n",
    "    trials = events.loc[events['event_type'] == 'trial'].copy().reset_index()\n",
    "    trial_data_list = []\n",
    "    for t, trial in events.groupby(\"trial_id\"):\n",
    "        trial_data = {'trial_id': t} | get_trial_data(trial)\n",
    "        trial_data_list.append(trial_data)\n",
    "    trial_data_df = pd.DataFrame(trial_data_list)\n",
    "    trials = pd.merge(trials, trial_data_df, on='trial_id')\n",
    "    trial['cons_length'] = trials['event_end_time'] - trials['cons_time']\n",
    "    return trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate spikes df for each unit\n",
    "get an array of spike times, making it into a df, then adding trial times to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spikes(spikes, trials):\n",
    "    spikes = pd.DataFrame(spikes, columns=['spike_time'])\n",
    "    for _, trial_basics in trials.iterrows():\n",
    "        trial_start_time = trial_basics['event_start_time']\n",
    "        trial_end_time = trial_basics['event_end_time']\n",
    "        spikes.loc[spikes['spike_time'].between(trial_start_time, trial_end_time), \n",
    "                'trial_id'] = trial_basics['trial_id']\n",
    "        spikes.loc[spikes['spike_time'].between(trial_start_time, trial_end_time), \n",
    "                'trial_time'] = spikes['spike_time'] - trial_start_time\n",
    "    return spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sorted_trial_id_dict(trials):\n",
    "    trial_id_by_time_waited = trials.sort_values(['wait_length']).trial_id.tolist()\n",
    "    trial_id_by_reward_and_time_watied = trials.sort_values(by=['missed','rewarded','wait_length']).trial_id.tolist()\n",
    "    trial_id_by_bg_length = trials.sort_values(['bg_length']).trial_id.tolist()\n",
    "    trial_id_by_trial_num = trials.trial_id.tolist()\n",
    "    sorted_trial_id_dict = {\n",
    "        \"time_waited\": trial_id_by_time_waited, \n",
    "        \"reward_time_waited\": trial_id_by_reward_and_time_watied, \n",
    "        \"bg_length\": trial_id_by_bg_length,\n",
    "        \"trial_num\": trial_id_by_trial_num\n",
    "    }\n",
    "    return sorted_trial_id_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting raster only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_figure():\n",
    "    fig = Figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    return fig, ax\n",
    "\n",
    "def align_times(trial_data, spikes, events, anchor):\n",
    "    anchor_time = trial_data[anchor].iloc[0]\n",
    "    aligned_spike_times = spikes['trial_time'] - anchor_time\n",
    "    \n",
    "    aligned_events = {}\n",
    "    for event_type in ['visual', 'wait', 'cons_reward', 'cons_no_reward']:\n",
    "        event_times = events.loc[events['event_type'] == event_type, 'event_start_trial_time']\n",
    "        aligned_events[event_type] = event_times - anchor_time\n",
    "    \n",
    "    return aligned_spike_times, aligned_events\n",
    "\n",
    "def plot_trial(ax, aligned_spike_times, aligned_events, trial_offset):\n",
    "    ax.eventplot(aligned_spike_times, lineoffsets=trial_offset, color='k', linelengths=1.0, linewidths=0.3)\n",
    "    \n",
    "    event_colors = {'visual': 'orange', 'wait': 'g', 'cons_reward': 'b', 'cons_no_reward': 'r'}\n",
    "    for event_type, times in aligned_events.items():\n",
    "        ax.eventplot(times, lineoffsets=trial_offset, color=event_colors[event_type], linelengths=1.0, linewidths=1)\n",
    "\n",
    "def add_legend(ax):\n",
    "    legend_handles = [\n",
    "        plt.Line2D([0], [0], color=c, lw=2) \n",
    "        for c in ['orange', 'g', 'b', 'r']\n",
    "    ]\n",
    "    legend_labels = ['Visual', 'Wait Start', 'Reward', 'No Reward']\n",
    "    ax.legend(legend_handles, legend_labels, loc='center left', bbox_to_anchor=(1, 0.95))\n",
    "\n",
    "def save_and_close_figure(fig, title, figure_folder):\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'{figure_folder}/{title}.png', bbox_inches='tight', dpi=300, format='png')\n",
    "    fig.clf()\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_raster_per_neuron(unit_identity, sorted_trial_id_dict, trials_by_trial, events_by_trial, spikes_by_trial, sorter, anchor, save_fig=True):\n",
    "    fig, ax = create_figure()\n",
    "    \n",
    "    trial_id_sorted = sorted_trial_id_dict[sorter]\n",
    "    plotted_trials = 0\n",
    "    \n",
    "    for t in trial_id_sorted:\n",
    "        if t in trials_by_trial.groups and t in spikes_by_trial.groups:\n",
    "            trial_data = trials_by_trial.get_group(t)\n",
    "            spikes = spikes_by_trial.get_group(t)\n",
    "            events = events_by_trial.get_group(t)\n",
    "            \n",
    "            aligned_spike_times, aligned_events = align_times(trial_data, spikes, events, anchor)\n",
    "            plot_trial(ax, aligned_spike_times, aligned_events, plotted_trials)\n",
    "            \n",
    "            if plotted_trials == 0:\n",
    "                add_legend(ax)\n",
    "            \n",
    "            plotted_trials += 1\n",
    "    \n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Trials')\n",
    "    title = f'{unit_identity} sorted by {sorter} aligned to {anchor}'\n",
    "    ax.set_title(title)\n",
    "    if save_fig:\n",
    "        save_and_close_figure(fig, title, figure_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_plots(trials_by_trial, sorted_trial_id_dict, events_by_trial, spikes_by_trial, unit_identity):\n",
    "    # plot_raster_per_neuron(unit_identity, sorted_trial_id_dict, trials_by_trial, events_by_trial, spikes_by_trial, 'time_waited', 'cue_on_time')\n",
    "    # plot_raster_per_neuron(unit_identity, sorted_trial_id_dict, trials_by_trial, events_by_trial, spikes_by_trial, 'bg_length', 'cue_on_time')\n",
    "    # plot_raster_per_neuron(unit_identity, sorted_trial_id_dict, trials_by_trial, events_by_trial, spikes_by_trial, 'trial_num', 'cue_on_time')\n",
    "    # plot_raster_per_neuron(unit_identity, sorted_trial_id_dict, trials_by_trial, events_by_trial, spikes_by_trial, 'reward_time_waited', 'cue_on_time')\n",
    "    \n",
    "    # plot_raster_per_neuron(unit_identity, sorted_trial_id_dict, trials_by_trial, events_by_trial, spikes_by_trial, 'time_waited', 'cue_off_time')\n",
    "    # plot_raster_per_neuron(unit_identity, sorted_trial_id_dict, trials_by_trial, events_by_trial, spikes_by_trial, 'bg_length', 'cue_off_time')\n",
    "    # plot_raster_per_neuron(unit_identity, sorted_trial_id_dict, trials_by_trial, events_by_trial, spikes_by_trial, 'trial_num', 'cue_off_time')\n",
    "    plot_raster_per_neuron(unit_identity, sorted_trial_id_dict, trials_by_trial, events_by_trial, spikes_by_trial, 'reward_time_waited', 'cue_off_time')\n",
    "\n",
    "    # plot_raster_per_neuron(unit_identity, sorted_trial_id_dict, trials_by_trial, events_by_trial, spikes_by_trial, 'time_waited', 'cons_time')\n",
    "    # plot_raster_per_neuron(unit_identity, sorted_trial_id_dict, trials_by_trial, events_by_trial, spikes_by_trial, 'bg_length', 'cons_time')\n",
    "    # plot_raster_per_neuron(unit_identity, sorted_trial_id_dict, trials_by_trial, events_by_trial, spikes_by_trial, 'trial_num', 'cons_time')\n",
    "    # plot_raster_per_neuron(unit_identity, sorted_trial_id_dict, trials_by_trial, events_by_trial, spikes_by_trial, 'reward_time_waited', 'cons_time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop through all sessions all units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_folder = os.path.join(figure_folder, \"raster_to_cue_off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa2004d014f44809c96a44e1b8fd49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Units:   0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fv/h96g2bdx42l2htl2pxpvr84h0000gn/T/ipykernel_11304/1350975543.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
      "/var/folders/fv/h96g2bdx42l2htl2pxpvr84h0000gn/T/ipykernel_11304/1350975543.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
      "/var/folders/fv/h96g2bdx42l2htl2pxpvr84h0000gn/T/ipykernel_11304/1350975543.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
      "/var/folders/fv/h96g2bdx42l2htl2pxpvr84h0000gn/T/ipykernel_11304/1350975543.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
      "/var/folders/fv/h96g2bdx42l2htl2pxpvr84h0000gn/T/ipykernel_11304/1350975543.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
      "/var/folders/fv/h96g2bdx42l2htl2pxpvr84h0000gn/T/ipykernel_11304/1350975543.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
      "/var/folders/fv/h96g2bdx42l2htl2pxpvr84h0000gn/T/ipykernel_11304/1350975543.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
      "/var/folders/fv/h96g2bdx42l2htl2pxpvr84h0000gn/T/ipykernel_11304/1350975543.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
      "/var/folders/fv/h96g2bdx42l2htl2pxpvr84h0000gn/T/ipykernel_11304/1350975543.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
      "/var/folders/fv/h96g2bdx42l2htl2pxpvr84h0000gn/T/ipykernel_11304/1350975543.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
      "/var/folders/fv/h96g2bdx42l2htl2pxpvr84h0000gn/T/ipykernel_11304/1350975543.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
      "/var/folders/fv/h96g2bdx42l2htl2pxpvr84h0000gn/T/ipykernel_11304/1350975543.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n"
     ]
    }
   ],
   "source": [
    "# claude code implementing tqdm\n",
    "total_units = sum(len(session['spikes']) for session in session_with_units)\n",
    "failed_units = []\n",
    "\n",
    "with tqdm(total=total_units, desc=\"Processing Units\") as pbar:\n",
    "    for session in session_with_units:\n",
    "        session_identity = generate_session_identity(session)\n",
    "        events = session['events']\n",
    "        events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
    "        events_by_trial = events.groupby('trial_id')\n",
    "        \n",
    "        trials = generate_trials(events)\n",
    "        sorted_trial_id_dict = generate_sorted_trial_id_dict(trials)\n",
    "        trials_by_trial = trials.groupby('trial_id')\n",
    "        \n",
    "        units = session['spikes']\n",
    "        for i, unit_spikes in enumerate(units):\n",
    "            unit_identity = session_identity + '_unit-' + str(i)\n",
    "            spikes = generate_spikes(unit_spikes, trials)\n",
    "            spikes_by_trial = spikes.groupby('trial_id')\n",
    "            try:\n",
    "                generate_all_plots(trials_by_trial, sorted_trial_id_dict, events_by_trial, spikes_by_trial, unit_identity)\n",
    "            except:\n",
    "                print(f\"Failed for: {unit_identity}\")\n",
    "                failed_units.append(unit_identity)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot raster with histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_to_anchor_times(df):\n",
    "    \"\"\"Add columns for different alignment times.\"\"\"\n",
    "    df['to_cue_on'] = df['trial_time'] - df['cue_on_time']\n",
    "    df['to_cue_off'] = df['trial_time'] - df['cue_off_time']\n",
    "    df['to_cons'] = df['trial_time'] - df['cons_time']\n",
    "    return df\n",
    "\n",
    "def prepare_aligned_data(events, trials, spikes):\n",
    "    \"\"\"Prepare aligned events and spikes data.\"\"\"\n",
    "    # Prepare events data\n",
    "    events_needed = ['visual', 'wait', 'cons_reward', 'cons_no_reward']\n",
    "    events_to_plot = events.loc[events['event_type'].isin(events_needed),\n",
    "                              ['trial_id', 'event_type', 'event_start_trial_time']]\n",
    "    \n",
    "    # Prepare trials data\n",
    "    trial_columns = ['trial_id', 'missed', 'rewarded', 'wait_length', \n",
    "                    'cue_on_time', 'cue_off_time', 'cons_time', 'bg_length']\n",
    "    trials_to_merge = trials[trial_columns].copy()\n",
    "    \n",
    "    # Align events\n",
    "    events_to_align = trials_to_merge.merge(events_to_plot, on='trial_id', how='inner')\n",
    "    events_to_align = events_to_align.rename(columns={'event_start_trial_time': 'trial_time'})\n",
    "    events_aligned = align_to_anchor_times(events_to_align)\n",
    "    \n",
    "    # Align spikes\n",
    "    spikes_to_align = trials_to_merge.merge(spikes, on='trial_id', how='inner')\n",
    "    spikes_aligned = align_to_anchor_times(spikes_to_align)\n",
    "    \n",
    "    return events_aligned, spikes_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_raster_legend(ax, event_colors):\n",
    "    \"\"\"Add legend to the raster plot.\"\"\"\n",
    "    legend_handles = [\n",
    "        plt.Line2D([0], [0], color=event_colors['visual'], lw=2),\n",
    "        plt.Line2D([0], [0], color=event_colors['wait'], lw=2),\n",
    "        plt.Line2D([0], [0], color=event_colors['cons_reward'], lw=2),\n",
    "        plt.Line2D([0], [0], color=event_colors['cons_no_reward'], lw=2)\n",
    "    ]\n",
    "    \n",
    "    legend_labels = ['Visual', 'Wait Start', 'Reward', 'No Reward']\n",
    "    \n",
    "    ax.legend(legend_handles, legend_labels, loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "\n",
    "def plot_raster(ax, spikes_aligned, events_aligned, sorted_trial_id_dict, \n",
    "                anchor, sorter, event_colors, histo_colors):\n",
    "    \"\"\"\n",
    "    Plot raster plot for spike times and events.\n",
    "    \"\"\"\n",
    "    spikes_aligned_by_trial = spikes_aligned.groupby('trial_id')\n",
    "    events_aligned_by_trial = events_aligned.groupby('trial_id')\n",
    "    plotted_trials = 0\n",
    "    \n",
    "    for t in sorted_trial_id_dict[sorter]:\n",
    "        if t in spikes_aligned_by_trial.groups:\n",
    "            # Plot spike times\n",
    "            trial_spike_times = spikes_aligned_by_trial.get_group(t)[anchor]\n",
    "            ax.eventplot(trial_spike_times, lineoffsets=plotted_trials, \n",
    "                        color='k', linelengths=1.0, linewidths=0.3)\n",
    "            \n",
    "            # Plot event times\n",
    "            trial_event_times = events_aligned_by_trial.get_group(t)[['event_type', anchor]]\n",
    "            trial_event_times_dict = trial_event_times.set_index('event_type')[anchor].to_dict()\n",
    "            \n",
    "            for event_type, event_time in trial_event_times_dict.items():\n",
    "                ax.eventplot([event_time], lineoffsets=plotted_trials,\n",
    "                           color=event_colors[event_type], linelengths=1.0, linewidths=1)\n",
    "            \n",
    "            if plotted_trials == 0:\n",
    "                add_raster_legend(ax, event_colors)\n",
    "            plotted_trials += 1\n",
    "    \n",
    "    # Add vertical line\n",
    "    ax.axvline(0, color=histo_colors[anchor], linestyle='--', alpha=0.5)\n",
    "    ax.set_ylabel('Trial number')\n",
    "\n",
    "def plot_kde(ax, spikes_aligned, anchor, histo_colors, event_colors):\n",
    "    \"\"\"\n",
    "    Plot KDE for spike times with common normalization between rewarded and non-rewarded.\n",
    "    \"\"\"\n",
    "    # Shared KDE parameters\n",
    "    kde_params = {\n",
    "        'bw_adjust': 0.5,\n",
    "        'common_norm': True,\n",
    "        'ax': ax\n",
    "    }\n",
    "    \n",
    "    # Plot KDEs\n",
    "    sns.kdeplot(spikes_aligned[anchor], color='black', label='All Spikes', \n",
    "                common_norm=False, bw_adjust=0.5, ax=ax)\n",
    "    sns.kdeplot(spikes_aligned.loc[spikes_aligned['rewarded']==True, anchor], \n",
    "                color=event_colors['cons_reward'], label='Rewarded', **kde_params)\n",
    "    sns.kdeplot(spikes_aligned.loc[spikes_aligned['rewarded']==False, anchor], \n",
    "                color=event_colors['cons_no_reward'], label='Non-Rewarded', **kde_params)\n",
    "    \n",
    "    # Add vertical line\n",
    "    ax.axvline(0, color=histo_colors[anchor], linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add legend and labels\n",
    "    ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "def save_and_close_figure(fig, title, figure_folder):\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'{figure_folder}/{title}.png', bbox_inches='tight', dpi=300, format='png')\n",
    "    fig.clf()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raster_and_kde(events, trials, spikes, sorted_trial_id_dict, \n",
    "                        unit_identity, anchor, sorter, figure_folder):\n",
    "    events_aligned, spikes_aligned = prepare_aligned_data(events, trials, spikes)\n",
    "\n",
    "    # Color schemes\n",
    "    histo_colors = {\n",
    "    'to_cue_on': 'lightcoral',\n",
    "    'to_cue_off': 'g',\n",
    "    'to_cons': 'mediumorchid'\n",
    "    }\n",
    "    event_colors = {\n",
    "    'visual': 'lightcoral',\n",
    "    'wait': 'g',\n",
    "    'cons_reward': 'tab:blue',\n",
    "    'cons_no_reward': 'tab:orange'\n",
    "    }\n",
    "\n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), height_ratios=[3, 1], sharex=True)\n",
    "    plt.subplots_adjust(hspace=0.1)\n",
    "    plot_raster(ax1, spikes_aligned, events_aligned, sorted_trial_id_dict, anchor, sorter, event_colors, histo_colors)\n",
    "    plot_kde(ax2, spikes_aligned, anchor, histo_colors, event_colors)\n",
    "\n",
    "        # Set title\n",
    "    title = f'{unit_identity} sorted by {sorter} aligned {anchor}'\n",
    "    ax1.set_title(title)\n",
    "\n",
    "    save_and_close_figure(fig, title, figure_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_plots(events, trials, spikes, sorted_trial_id_dict, unit_identity, figure_folder):\n",
    "    plot_raster_and_kde(events, trials, spikes, sorted_trial_id_dict, unit_identity, \n",
    "                   'to_cue_on', 'reward_time_waited', figure_folder)\n",
    "    plot_raster_and_kde(events, trials, spikes, sorted_trial_id_dict, unit_identity, \n",
    "                   'to_cue_off', 'reward_time_waited', figure_folder)\n",
    "    plot_raster_and_kde(events, trials, spikes, sorted_trial_id_dict, unit_identity, \n",
    "                   'to_cons', 'reward_time_waited', figure_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single session test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = session_with_units[0]\n",
    "session_identity = generate_session_identity(session)\n",
    "events = session['events']\n",
    "events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
    "\n",
    "trials = generate_trials(events)\n",
    "sorted_trial_id_dict = generate_sorted_trial_id_dict(trials)\n",
    "\n",
    "units = session['spikes']\n",
    "i=4\n",
    "unit_spikes = units[i]\n",
    "unit_identity = session_identity + '_unit-' + str(i)\n",
    "spikes = generate_spikes(unit_spikes, trials)\n",
    "events_aligned, spikes_aligned = prepare_aligned_data(events, trials, spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histo_colors = {\n",
    "#     'to_cue_on': 'lightcoral',\n",
    "#     'to_cue_off': 'g',\n",
    "#     'to_cons': 'mediumorchid'\n",
    "# }\n",
    "# event_colors = {\n",
    "#     'visual': 'lightcoral',\n",
    "#     'wait': 'g',\n",
    "#     'cons_reward': 'tab:blue',\n",
    "#     'cons_no_reward': 'tab:orange'\n",
    "# }\n",
    "# anchor = 'to_cue_on'\n",
    "# sorter = 'reward_time_waited'\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), height_ratios=[3, 1], sharex=True)\n",
    "# plt.subplots_adjust(hspace=0.1)\n",
    "\n",
    "# plot_raster(ax1, spikes_aligned, events_aligned, sorted_trial_id_dict, anchor, sorter, event_colors, histo_colors)\n",
    "# plot_kde(ax2, spikes_aligned, anchor, histo_colors, event_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop through all sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_units = sum(len(session['spikes']) for session in session_with_units)\n",
    "failed_units = []\n",
    "\n",
    "with tqdm(total=total_units, desc=\"Processing Units\") as pbar:\n",
    "    for session in session_with_units:\n",
    "        session_identity = generate_session_identity(session)\n",
    "        events = session['events']\n",
    "        events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
    "\n",
    "        trials = generate_trials(events)\n",
    "        sorted_trial_id_dict = generate_sorted_trial_id_dict(trials)\n",
    "        \n",
    "        units = session['spikes']\n",
    "        for i, unit_spikes in enumerate(units):\n",
    "            unit_identity = session_identity + '_unit-' + str(i)\n",
    "            spikes = generate_spikes(unit_spikes, trials)\n",
    "            try:\n",
    "                generate_all_plots(events, trials, spikes, sorted_trial_id_dict, unit_identity, figure_folder)\n",
    "            except:\n",
    "                print(f\"Failed for: {unit_identity}\")\n",
    "                failed_units.append(unit_identity)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single session test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neo\n",
    "import quantities as pq\n",
    "from elephant.statistics import instantaneous_rate\n",
    "from elephant.kernels import GaussianKernel\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_to_anchor_times(df):\n",
    "    \"\"\"Add columns for different alignment times.\"\"\"\n",
    "    df['to_cue_on'] = df['trial_time'] - df['cue_on_time']\n",
    "    df['to_cue_off'] = df['trial_time'] - df['cue_off_time']\n",
    "    df['to_cons'] = df['trial_time'] - df['cons_time']\n",
    "    return df\n",
    "\n",
    "def assign_period(row):\n",
    "    if row['cue_on_time'] <= row['trial_time'] < row['cue_off_time']:\n",
    "        return 'background'\n",
    "    elif row['cue_off_time'] <= row['trial_time'] < row['cons_time']:\n",
    "        return 'wait'\n",
    "    elif row['cons_time'] <= row['trial_time']:\n",
    "        return 'cons'\n",
    "\n",
    "def prepare_aligned_data(events, trials, spikes):\n",
    "    \"\"\"Prepare aligned events and spikes data.\"\"\"\n",
    "    # Prepare events data\n",
    "    events_needed = ['visual', 'wait', 'cons_reward', 'cons_no_reward']\n",
    "    events_to_plot = events.loc[events['event_type'].isin(events_needed),\n",
    "                              ['trial_id', 'event_type', 'event_start_trial_time']]\n",
    "    \n",
    "    # Prepare trials data\n",
    "    trial_columns = ['trial_id', 'trial_length', 'missed', 'rewarded',\n",
    "                     'bg_length', 'wait_length', \n",
    "                    'cue_on_time', 'cue_off_time', 'cons_time']\n",
    "    trials_to_merge = trials[trial_columns].copy()\n",
    "    \n",
    "    # Align events\n",
    "    events_to_align = trials_to_merge.merge(events_to_plot, on='trial_id', how='inner')\n",
    "    events_to_align = events_to_align.rename(columns={'event_start_trial_time': 'trial_time'})\n",
    "    events_aligned = align_to_anchor_times(events_to_align)\n",
    "    \n",
    "    # Align spikes\n",
    "    spikes_to_align = trials_to_merge.merge(spikes, on='trial_id', how='inner')\n",
    "    spikes_aligned = align_to_anchor_times(spikes_to_align)\n",
    "    spikes_aligned['period'] = spikes_aligned.apply(assign_period, axis=1)\n",
    "    \n",
    "    return events_aligned, spikes_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = session_with_units[0]\n",
    "session_identity = generate_session_identity(session)\n",
    "events = session['events']\n",
    "events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
    "\n",
    "# trials = generate_trials(events)\n",
    "# sorted_trial_id_dict = generate_sorted_trial_id_dict(trials)\n",
    "\n",
    "# units = session['spikes']\n",
    "# i=2\n",
    "# unit_spikes = units[i]\n",
    "# unit_identity = session_identity + '_unit-' + str(i)\n",
    "# spikes = generate_spikes(unit_spikes, trials)\n",
    "\n",
    "# events_aligned, spikes_aligned = prepare_aligned_data(events, trials, spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot sessiong FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your raw spike times (in seconds)\n",
    "spike_times = spikes.spike_time.tolist()  # e.g., [0.1, 0.3, 0.7, 0.9]\n",
    "\n",
    "# Define the trial duration (t_stop) for the SpikeTrain\n",
    "t_stop = max(spike_times) + 1.0  # Add buffer (adjust as needed)\n",
    "\n",
    "# Create a neo.SpikeTrain\n",
    "spike_train = neo.SpikeTrain(\n",
    "    spike_times,\n",
    "    units=pq.s,        # Units of spike times (seconds)\n",
    "    t_start=0.0 * pq.s,  # Optional: session start time\n",
    "    t_stop=t_stop * pq.s  # session end time\n",
    ")\n",
    "\n",
    "# Define kernel (e.g., Gaussian with σ=100ms)\n",
    "kernel = GaussianKernel(sigma=2 * pq.s)\n",
    "\n",
    "# Calculate instantaneous rate\n",
    "rate = instantaneous_rate(\n",
    "    spike_train,\n",
    "    sampling_period=10 * pq.ms,  # 10ms bins\n",
    "    kernel=kernel\n",
    ")\n",
    "\n",
    "# Get the correct time axis from the rate object itself\n",
    "time_axis = rate.times.rescale('s').magnitude  # Convert to seconds as numpy array\n",
    "rate_array = rate.magnitude.squeeze()\n",
    "\n",
    "# Now plot (dimensions will match)\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(time_axis, rate_array)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.xlim(0, time_axis.max())\n",
    "plt.ylabel(\"Firing rate (Hz)\")\n",
    "plt.title(\"Instantaneous Firing Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_session_mean=rate_array.mean()\n",
    "fr_session_max=rate_array.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting histo with corrected bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "histo without error bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_instantaneous_rate(spike_train, sampling_period=0.01, sigma=0.3, bounds=None):\n",
    "    \"\"\"\n",
    "    Calculate instantaneous firing rate using Gaussian kernel smoothing.\n",
    "    \n",
    "    Parameters:\n",
    "        spike_train (array): Array of spike times\n",
    "        sampling_period (float): Time resolution for rate calculation (in seconds)\n",
    "        sigma (float): Standard deviation of Gaussian kernel (in seconds)\n",
    "        bounds (tuple): (t_start, t_stop) time window to analyze\n",
    "        test_plot (bool): Whether to plot a test snippet\n",
    "        \n",
    "    Returns:\n",
    "        rate_array (array): Instantaneous firing rate (Hz)\n",
    "        time_axis (array): Corresponding time points\n",
    "    \"\"\"\n",
    "    # Set time bounds\n",
    "    t_start, t_stop = bounds if bounds is not None else (0, spike_train[-1])\n",
    "    \n",
    "    # Create time axis (centered on bins)\n",
    "    time_axis = np.arange(t_start, t_stop, sampling_period) + sampling_period / 2\n",
    "    \n",
    "    # Filter spikes within bounds and create SpikeTrain object\n",
    "    spike_train = spike_train[(t_start < spike_train) & (spike_train < t_stop)]\n",
    "    spike_train = neo.SpikeTrain(spike_train, t_stop=t_stop, units=pq.s)\n",
    "    \n",
    "    # Calculate instantaneous rate\n",
    "    kernel = GaussianKernel(sigma=sigma * pq.s)\n",
    "    rate = instantaneous_rate(\n",
    "        spike_train,\n",
    "        sampling_period=sampling_period * pq.s,\n",
    "        kernel=kernel,\n",
    "        t_start=t_start * pq.s,\n",
    "        t_stop=t_stop * pq.s,\n",
    "        border_correction=True\n",
    "    )\n",
    "    \n",
    "    return time_axis, rate.as_array().squeeze()\n",
    "\n",
    "def count_active_trials(trial_durations, bin_edges):\n",
    "    \"\"\"\n",
    "    Count how many trials are active (ongoing) at each time bin.\n",
    "    \n",
    "    Parameters:\n",
    "        trial_durations (array): Duration of each trial\n",
    "        bin_edges (array): Edges of time bins\n",
    "        \n",
    "    Returns:\n",
    "        active_counts (array): Number of active trials per bin\n",
    "    \"\"\"\n",
    "    return [np.sum(trial_durations > edge) for edge in bin_edges[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_event = 'to_cue_on'\n",
    "max_trial_length = round(spikes_aligned.trial_length.max(), 1)\n",
    "time_step = 0.1\n",
    "\n",
    "# Define analysis window\n",
    "analysis_bounds = (0, max_trial_length)\n",
    "time_bins = np.arange(analysis_bounds[0], analysis_bounds[1] + time_step, time_step)\n",
    "\n",
    "# Get trial durations and active trial counts\n",
    "trial_durations = spikes_aligned.groupby('trial_id')['trial_length'].first()\n",
    "active_trials = count_active_trials(trial_durations, time_bins)\n",
    "\n",
    "# Calculate and normalize firing rate\n",
    "time_axis, firing_rate = calculate_instantaneous_rate(\n",
    "    spikes_aligned[anchor_event],\n",
    "    bounds=analysis_bounds,\n",
    "    sampling_period=time_step,\n",
    "    sigma=0.3\n",
    ")\n",
    "\n",
    "normalized_rate = firing_rate / active_trials\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(time_axis, normalized_rate)\n",
    "plt.xlabel('Time from cue onset (s)')\n",
    "plt.ylabel('Normalized firing rate (Hz)')\n",
    "plt.title('Firing rate aligned to cue onset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate not corrected\n",
    "bin_org = np.full(len(time_bins)-1, spikes_aligned.trial_id.max()+1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(time_axis, firing_rate / bin_org)\n",
    "plt.xlabel('Time from cue onset (s)')\n",
    "plt.ylabel('Normalized firing rate (Hz)')\n",
    "plt.title('Firing rate aligned to cue onset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "histo with error bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_by_trial = spikes_aligned.groupby('trial_id')\n",
    "bin_centers = time_bins[:-1] + time_step/2\n",
    "active_trials = count_active_trials(trial_durations, time_bins)\n",
    "active_trials = np.array(active_trials)\n",
    "\n",
    "all_trial_spike_times = []\n",
    "for t, trial in spikes_by_trial:\n",
    "    all_trial_spike_times.append(trial[anchor_event].tolist())\n",
    "\n",
    "# Initialize empty array (trials × bins)\n",
    "counts_per_trial = np.zeros((len(all_trial_spike_times), len(time_bins)-1))\n",
    "rates_per_trial = np.zeros_like(counts_per_trial) #[trial, time_bin]\n",
    "\n",
    "for i, trial_spikes in enumerate(all_trial_spike_times):\n",
    "    counts_per_trial[i], _ = np.histogram(trial_spikes, bins=time_bins)\n",
    "    rates_per_trial[i] = counts_per_trial[i] / time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing code up to SEM calculation\n",
    "rates_all_trials = np.nansum(rates_per_trial, 0)/active_trials \n",
    "rates_all_trials_sem = np.std(rates_per_trial, axis=0)/np.sqrt(active_trials)\n",
    "\n",
    "# Create mask for bins with at least 5 active trials\n",
    "trial_duration_mask = 5\n",
    "\n",
    "valid_bins_mask = active_trials >= trial_duration_mask  # Boolean array where True means ≥5 trials\n",
    "\n",
    "# Apply smoothing only to valid data to avoid edge artifacts\n",
    "smoothed_rates = np.full_like(rates_all_trials, np.nan)  # Initialize with NaNs\n",
    "smoothed_sem = np.full_like(rates_all_trials_sem, np.nan)\n",
    "\n",
    "# Smooth only the valid portions (preserves original data length)\n",
    "sigma = 0.5\n",
    "smoothed_rates[valid_bins_mask] = gaussian_filter1d(rates_all_trials[valid_bins_mask], sigma=sigma)\n",
    "smoothed_sem[valid_bins_mask] = gaussian_filter1d(rates_all_trials_sem[valid_bins_mask], sigma=sigma)\n",
    "\n",
    "# Get bin centers only for valid bins\n",
    "valid_bin_centers = bin_centers[valid_bins_mask]\n",
    "valid_smoothed_rates = smoothed_rates[valid_bins_mask]\n",
    "valid_smoothed_sem = smoothed_sem[valid_bins_mask]\n",
    "\n",
    "# Plot only valid bins\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(valid_bin_centers, valid_smoothed_rates, 'b-', lw=2, label='Mean rate (≥5 trials)')\n",
    "plt.fill_between(\n",
    "    valid_bin_centers,\n",
    "    valid_smoothed_rates - valid_smoothed_sem,\n",
    "    valid_smoothed_rates + valid_smoothed_sem,\n",
    "    color='blue',\n",
    "    alpha=0.3,\n",
    "    label='± SEM'\n",
    ")\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Firing rate (Hz)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fix it to sweep both forward and backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUE_ON = 'cue_on_time'\n",
    "CUE_OFF = 'cue_off_time'\n",
    "CONS = 'cons_time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = session_with_units[0]\n",
    "session_identity = generate_session_identity(session)\n",
    "\n",
    "events = session['events']\n",
    "events = events.groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
    "\n",
    "trials = generate_trials(events)\n",
    "\n",
    "units = session['spikes']\n",
    "i=2\n",
    "unit_spikes = units[i]\n",
    "unit_identity = session_identity + '_unit-' + str(i)\n",
    "spikes = generate_spikes(unit_spikes, trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = CUE_ON\n",
    "time_step = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials['aligned_start_time']=trials['event_start_trial_time']-trials[anchor]\n",
    "trials['aligned_end_time']=trials['event_end_trial_time']-trials[anchor]\n",
    "bounds = (\n",
    "    round(trials.aligned_start_time.min(), 1), \n",
    "    round(trials.aligned_end_time.max(), 1)\n",
    "    )\n",
    "bin_edges = np.arange(bounds[0] - time_step, bounds[1] + time_step, time_step) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_trials_per_bin = np.zeros(len(bin_edges) - 1, dtype=int)\n",
    "\n",
    "for _, trial in trials.iterrows():\n",
    "    start = trial['aligned_start_time']\n",
    "    end = trial['aligned_end_time']\n",
    "    \n",
    "    # Find bins that overlap with this trial's duration\n",
    "    overlaps = (bin_edges[:-1] < end) & (bin_edges[1:] > start)\n",
    "    active_trials_per_bin[overlaps] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_centers = bin_edges[:-1] + time_step/2\n",
    "bin_centers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_columns = ['trial_id', anchor]\n",
    "trials_to_merge = trials[trial_columns].copy()\n",
    "\n",
    "# Align spikes\n",
    "spikes = trials_to_merge.merge(spikes, on='trial_id', how='inner')\n",
    "spikes['aligned_time'] = spikes['trial_time'] - spikes[anchor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_by_trial = spikes.groupby('trial_id')\n",
    "\n",
    "all_trial_spike_times = []\n",
    "for t, trial in spikes_by_trial:\n",
    "    all_trial_spike_times.append(trial.aligned_time.tolist())\n",
    "\n",
    "# Initialize empty array (trials × bins)\n",
    "counts_per_trial = np.zeros((len(all_trial_spike_times), len(bin_edges)-1))\n",
    "rates_per_trial = np.zeros_like(counts_per_trial) #[trial, time_bin]\n",
    "\n",
    "for i, trial_spikes in enumerate(all_trial_spike_times):\n",
    "    counts_per_trial[i], _ = np.histogram(trial_spikes, bins=bin_edges)\n",
    "    rates_per_trial[i] = counts_per_trial[i] / time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing code up to SEM calculation\n",
    "rates_all_trials = np.nansum(rates_per_trial, 0)/active_trials_per_bin \n",
    "rates_all_trials_sem = np.std(rates_per_trial, axis=0)/np.sqrt(active_trials_per_bin)\n",
    "\n",
    "# Create mask for bins with at least 5 active trials\n",
    "trial_duration_mask = 5\n",
    "\n",
    "valid_bins_mask = active_trials_per_bin >= trial_duration_mask  # Boolean array where True means ≥5 trials\n",
    "\n",
    "# Apply smoothing only to valid data to avoid edge artifacts\n",
    "smoothed_rates = np.full_like(rates_all_trials, np.nan)  # Initialize with NaNs\n",
    "smoothed_sem = np.full_like(rates_all_trials_sem, np.nan)\n",
    "\n",
    "# Smooth only the valid portions (preserves original data length)\n",
    "sigma = 0.5\n",
    "smoothed_rates[valid_bins_mask] = gaussian_filter1d(rates_all_trials[valid_bins_mask], sigma=sigma)\n",
    "smoothed_sem[valid_bins_mask] = gaussian_filter1d(rates_all_trials_sem[valid_bins_mask], sigma=sigma)\n",
    "\n",
    "# Get bin centers only for valid bins\n",
    "valid_bin_centers = bin_centers[valid_bins_mask]\n",
    "valid_smoothed_rates = smoothed_rates[valid_bins_mask]\n",
    "valid_smoothed_sem = smoothed_sem[valid_bins_mask]\n",
    "\n",
    "# Plot only valid bins\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(valid_bin_centers, valid_smoothed_rates, 'b-', lw=2, label='Mean rate (≥5 trials)')\n",
    "plt.fill_between(\n",
    "    valid_bin_centers,\n",
    "    valid_smoothed_rates - valid_smoothed_sem,\n",
    "    valid_smoothed_rates + valid_smoothed_sem,\n",
    "    color='blue',\n",
    "    alpha=0.3,\n",
    "    label='± SEM'\n",
    ")\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Firing rate (Hz)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "organization attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 1. Configuration\n",
    "# ==============================================\n",
    "CUE_ON = 'cue_on_time'\n",
    "CUE_OFF = 'cue_off_time'\n",
    "CONS = 'cons_time'\n",
    "\n",
    "# ==============================================\n",
    "# 2. Data Preparation\n",
    "# ==============================================\n",
    "def prepare_data(session, unit_idx=2):\n",
    "    \"\"\"Organizes spikes and trials into aligned time bins.\"\"\"\n",
    "    # Get session data\n",
    "    events = session['events'].groupby('trial_id', group_keys=False).apply(add_trial_time)\n",
    "    trials = generate_trials(events)\n",
    "    spikes = generate_spikes(session['spikes'][unit_idx], trials)\n",
    "    \n",
    "    # Align times to anchor event\n",
    "    trials['aligned_start_time'] = trials['event_start_trial_time'] - trials[anchor]\n",
    "    trials['aligned_end_time'] = trials['event_end_trial_time'] - trials[anchor]\n",
    "\n",
    "    trial_columns = ['trial_id', anchor]\n",
    "    trials_to_merge = trials[trial_columns].copy()\n",
    "\n",
    "    # Align spikes\n",
    "    spikes = trials_to_merge.merge(spikes, on='trial_id', how='inner')\n",
    "    spikes['aligned_time'] = spikes['trial_time'] - spikes[anchor]\n",
    "    \n",
    "    # Calculate bounds rounded to time_step precision\n",
    "    bounds = (\n",
    "        np.round(trials.aligned_start_time.min(), decimals=1),\n",
    "        np.round(trials.aligned_end_time.max(), decimals=1)\n",
    "    )\n",
    "    \n",
    "    # Create bins with buffer for edge cases\n",
    "    bin_edges = np.arange(\n",
    "        bounds[0] - time_step,\n",
    "        bounds[1] + 2*time_step,  # Extra buffer for safety\n",
    "        time_step\n",
    "    )\n",
    "    \n",
    "    return trials, spikes, bin_edges\n",
    "\n",
    "# ==============================================\n",
    "# 3. Active Trial Calculation\n",
    "# ==============================================\n",
    "def calculate_active_trials(trials, bin_edges):\n",
    "    \"\"\"Counts active trials for each time bin.\"\"\"\n",
    "    active_trials = np.zeros(len(bin_edges) - 1, dtype=int)\n",
    "    \n",
    "    for _, trial in trials.iterrows():\n",
    "        overlaps = (bin_edges[:-1] < trial['aligned_end_time']) & \\\n",
    "                   (bin_edges[1:] > trial['aligned_start_time'])\n",
    "        active_trials[overlaps] += 1\n",
    "    \n",
    "    return active_trials\n",
    "\n",
    "# ==============================================\n",
    "# 4. Rate Calculation\n",
    "# ==============================================\n",
    "def calculate_rates(spikes, bin_edges, active_trials):\n",
    "    \"\"\"Calculates firing rates with safe division.\"\"\"\n",
    "    # Bin spikes across trials\n",
    "    counts = np.array([\n",
    "        np.histogram(trial['aligned_time'], bins=bin_edges)[0]\n",
    "        for _, trial in spikes.groupby('trial_id')\n",
    "    ])\n",
    "    \n",
    "    # Convert to rates (Hz)\n",
    "    rates = counts / time_step\n",
    "    \n",
    "    # Safe division with error handling\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        mean_rates = np.nansum(rates, axis=0) / active_trials\n",
    "        sem_rates = np.std(rates, axis=0) / np.sqrt(active_trials)\n",
    "        \n",
    "        mean_rates = np.nan_to_num(mean_rates, nan=0.0)\n",
    "        sem_rates = np.nan_to_num(sem_rates, nan=0.0)\n",
    "    \n",
    "    return mean_rates, sem_rates\n",
    "\n",
    "# ==============================================\n",
    "# 5. Main Analysis Pipeline\n",
    "# ==============================================\n",
    "def analyze_unit(session, unit_idx=2):\n",
    "    # Prepare data\n",
    "    trials, spikes, bin_edges = prepare_data(session, unit_idx)\n",
    "    \n",
    "    # Calculate active trials\n",
    "    active_trials = calculate_active_trials(trials, bin_edges)\n",
    "    \n",
    "    # Calculate rates\n",
    "    mean_rates, sem_rates = calculate_rates(spikes, bin_edges, active_trials)\n",
    "    \n",
    "    # Apply smoothing to valid bins only\n",
    "    valid_mask = active_trials >= min_trials\n",
    "    bin_centers = bin_edges[:-1] + time_step/2\n",
    "    \n",
    "    smoothed_rates = np.full_like(mean_rates, np.nan)\n",
    "    smoothed_sem = np.full_like(sem_rates, np.nan)\n",
    "    \n",
    "    smoothed_rates[valid_mask] = gaussian_filter1d(mean_rates[valid_mask], sigma=sigma)\n",
    "    smoothed_sem[valid_mask] = gaussian_filter1d(sem_rates[valid_mask], sigma=sigma)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(bin_centers[valid_mask], smoothed_rates[valid_mask], 'b-', lw=2, \n",
    "             label=f'Mean rate (≥{min_trials} trials)')\n",
    "    plt.fill_between(\n",
    "        bin_centers[valid_mask],\n",
    "        smoothed_rates[valid_mask] - smoothed_sem[valid_mask],\n",
    "        smoothed_rates[valid_mask] + smoothed_sem[valid_mask],\n",
    "        color='blue', alpha=0.3, label='± SEM'\n",
    "    )\n",
    "    plt.axvline(0, color='k', linestyle='--', label=anchor)\n",
    "    plt.xlabel('Time relative to event (s)')\n",
    "    plt.ylabel('Firing rate (Hz)')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# # ==============================================\n",
    "# # Execute Analysis\n",
    "# # ==============================================\n",
    "# analyze_unit(session_with_units[0], unit_idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = CUE_ON\n",
    "time_step = 0.1\n",
    "trials, spikes, bin_edges = prepare_data(session, unit_idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = CUE_ON\n",
    "time_step = 0.1\n",
    "min_trials = 5\n",
    "sigma = 3\n",
    "analyze_unit(session_with_units[0], unit_idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = CUE_OFF\n",
    "time_step = 0.1\n",
    "min_trials = 5\n",
    "sigma = 3\n",
    "analyze_unit(session_with_units[0], unit_idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = CONS\n",
    "time_step = 0.1\n",
    "min_trials = 5\n",
    "sigma = 3\n",
    "analyze_unit(session_with_units[0], unit_idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CHEERIOS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
